{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "\n",
    "This sample is taken from the Keras repo:\n",
    "https://github.com/keras-team/keras\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This step imports the needed keras functions.\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "\n",
    "# This step imports the actual data which is available in keras as as demo.\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# This will import the model type of Sequential which will be used as a scaffold for the sequential model.\n",
    "from keras.models import Sequential\n",
    "\n",
    "# These layer types will be added to the sequential model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# These layer types will also be added to create further features from the available data.\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Create a variable called batch_size and set it to 128, create a variable num_classes and set it to 10, create a variable epochs and set it to 3.\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Create a variable called img_rows and img_cols and assign them both the value of 28\n",
    "\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mnist library will return arrays with the load_data method.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# These datasets have already been split into test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is the type of the x_train object?\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the shape (how many dimensions) is the x_train object?\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this is 60 thousand images that are 28 by 28 pixels.\n",
    "# Since the pictures are black and white each number is a pixel between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If white is 0 and black is 255 then this should look like a gradient when it is converted to pixels.\n",
    "gradient = [i for i in range(28*28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783]\n"
     ]
    }
   ],
   "source": [
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We should reshape the gradient so it is line by line - just like in the sample - so a 28x28 box instead of one 784 line.\n",
    "\n",
    "gradient_box = np.array(gradient).reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b8f4b38>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(gradient_box, cmap=plt.get_cmap('gray_r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function that will take a sample of the dataset and display what it looks like and then show what the 'ground truth' or real label is\n",
    "def plot_digit(num_sample):\n",
    "    # Get the label of the image.\n",
    "    label = y_train[num_sample]\n",
    "    # Get the actual pixels of the image\n",
    "    img = x_train[num_sample].reshape([28,28])\n",
    "    #Set the title of the sample number and what the label is\n",
    "    plt.title('Sample: {0}  Groundtruth label: {1}'.format(num_sample, label))\n",
    "    #Create a visual of the image\n",
    "    plt.imshow(img, cmap=plt.get_cmap('gray_r'))\n",
    "    #Show the image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for x in x_train[22].reshape(28,28):\n",
    "    print([min(1,int(a)) for a in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step will reshape the data structures so they are in the right format for the model to train.\n",
    "# Remember the img_rows and img_cols was set above to 28 and 28\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So this is 28 by 28 by 1 pixel\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"So this is {0} by {1} by {2} pixel\"\"\".format(input_shape[0], input_shape[1], input_shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please se the types of the two x arrays as float32\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next function will normalise the data.\n",
    "# Normalisation means that we can constrain the boundaries between 0 and 1.\n",
    "# This function does this simply.\n",
    "\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "# This means that there are 60 thousands samples of 28x28 arrays with one value each.\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next section is about actually creating the model and starting training now that the data is prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# This object accepts a number of different layers in the order that they are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first layer is a two dimension convolutional layer that is 3x3 in size with an activation function of 'relu'\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Conv](convolution.gif \"Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second layer is doing this again.\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third layer is pooling - it is grabbing the maximum value of each 4x4 square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![football](figure.gif \"Pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forth layer is running dropout to remove some values randomly from the training. The idea around this is to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The layer is then flattened.\n",
    "# This reshapes the array to be one long line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This condensces down to 128 nodes\n",
    "model.add(Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANother dropout layer to reduce overfitting\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is then reduced to predict one of the 10 classes using softmax.\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The compile function sets the loss, optimiser and metrics that will be produced.\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      " 8960/60000 [===>..........................] - ETA: 1:48 - loss: 0.5315 - acc: 0.8304"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-53a6169058d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This will start to train the model on the x,y train datasets.\n",
    "# The number of records fed in at a time is the batch size.\n",
    "# The epoch - or number of times the full sample will be pushed through the network is set.\n",
    "# Verbose shows a nice processing output\n",
    "# The validation data is provided so each epoch will have accuracy results\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
